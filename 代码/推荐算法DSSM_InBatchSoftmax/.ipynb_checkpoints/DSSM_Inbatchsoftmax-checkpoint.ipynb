{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b38f030",
   "metadata": {},
   "source": [
    "参考文档 https://deepmatch.readthedocs.io/en/latest/Features.html\n",
    "参考代码 https://github.com/shenweichen/DeepMatch/blob/master/examples/colab_MovieLen1M_DSSM_InBatchSoftmax.ipynb\n",
    "不考虑代码冗余的情况，熟悉tensorflow环境的背景下，重复用几遍。后面再考虑抽象公共函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0618d3c",
   "metadata": {},
   "source": [
    "# 1、处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b81ed10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000199</th>\n",
       "      <td>5334</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>46140</td>\n",
       "      <td>3382</td>\n",
       "      <td>5</td>\n",
       "      <td>960796159</td>\n",
       "      <td>Song of Freedom (1936)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000200</th>\n",
       "      <td>5420</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>14850</td>\n",
       "      <td>1843</td>\n",
       "      <td>3</td>\n",
       "      <td>960156505</td>\n",
       "      <td>Slappy and the Stinkers (1998)</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000201</th>\n",
       "      <td>5433</td>\n",
       "      <td>F</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>45014</td>\n",
       "      <td>286</td>\n",
       "      <td>3</td>\n",
       "      <td>960240881</td>\n",
       "      <td>Nemesis 2: Nebula (1995)</td>\n",
       "      <td>Action|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000202</th>\n",
       "      <td>5494</td>\n",
       "      <td>F</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>94306</td>\n",
       "      <td>3530</td>\n",
       "      <td>4</td>\n",
       "      <td>959816296</td>\n",
       "      <td>Smoking/No Smoking (1993)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000203</th>\n",
       "      <td>5556</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>92103</td>\n",
       "      <td>2198</td>\n",
       "      <td>3</td>\n",
       "      <td>959445515</td>\n",
       "      <td>Modulations (1998)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>5949</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>47901</td>\n",
       "      <td>2198</td>\n",
       "      <td>5</td>\n",
       "      <td>958846401</td>\n",
       "      <td>Modulations (1998)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>5675</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>30030</td>\n",
       "      <td>2703</td>\n",
       "      <td>3</td>\n",
       "      <td>976029116</td>\n",
       "      <td>Broken Vessels (1998)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>5780</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>92886</td>\n",
       "      <td>2845</td>\n",
       "      <td>1</td>\n",
       "      <td>958153068</td>\n",
       "      <td>White Boys (1999)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>5851</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>55410</td>\n",
       "      <td>3607</td>\n",
       "      <td>5</td>\n",
       "      <td>957756608</td>\n",
       "      <td>One Little Indian (1973)</td>\n",
       "      <td>Comedy|Drama|Western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>5938</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>35401</td>\n",
       "      <td>2909</td>\n",
       "      <td>4</td>\n",
       "      <td>957273353</td>\n",
       "      <td>Five Wives, Three Secretaries and Me (1998)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id gender  age  occupation    zip  movie_id  rating  timestamp  \\\n",
       "1000199     5334      F   56          13  46140      3382       5  960796159   \n",
       "1000200     5420      F    1          19  14850      1843       3  960156505   \n",
       "1000201     5433      F   35          17  45014       286       3  960240881   \n",
       "1000202     5494      F   35          17  94306      3530       4  959816296   \n",
       "1000203     5556      M   45           6  92103      2198       3  959445515   \n",
       "1000204     5949      M   18          17  47901      2198       5  958846401   \n",
       "1000205     5675      M   35          14  30030      2703       3  976029116   \n",
       "1000206     5780      M   18          17  92886      2845       1  958153068   \n",
       "1000207     5851      F   18          20  55410      3607       5  957756608   \n",
       "1000208     5938      M   25           1  35401      2909       4  957273353   \n",
       "\n",
       "                                               title                  genres  \n",
       "1000199                       Song of Freedom (1936)                   Drama  \n",
       "1000200               Slappy and the Stinkers (1998)       Children's|Comedy  \n",
       "1000201                     Nemesis 2: Nebula (1995)  Action|Sci-Fi|Thriller  \n",
       "1000202                    Smoking/No Smoking (1993)                  Comedy  \n",
       "1000203                           Modulations (1998)             Documentary  \n",
       "1000204                           Modulations (1998)             Documentary  \n",
       "1000205                        Broken Vessels (1998)                   Drama  \n",
       "1000206                            White Boys (1999)                   Drama  \n",
       "1000207                     One Little Indian (1973)    Comedy|Drama|Western  \n",
       "1000208  Five Wives, Three Secretaries and Me (1998)             Documentary  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path=\"../推荐算法-双塔/ml-1m\"\n",
    "unames = ['user_id','gender', 'age','occupation','zip']\n",
    "user_df = pd.read_csv(data_path+'/users.dat', sep='::',\n",
    "                      engine=\"python\",\n",
    "                      encoding='iso-8859-1',\n",
    "                      header=None, names=unames)\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "rating_df = pd.read_csv(data_path+'/ratings.dat', sep='::',\n",
    "                     engine='python',encoding='iso-8859-1',\n",
    "                     header=None, names=rnames)\n",
    "mnames = ['movie_id','title','genres']\n",
    "movie_df = pd.read_csv(data_path+'/movies.dat',sep='::',\n",
    "                    engine=\"python\",encoding='iso-8859-1',\n",
    "                    header=None, names=mnames)\n",
    "data = pd.merge(pd.merge(user_df, rating_df), movie_df)\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010ec424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c36b5",
   "metadata": {},
   "source": [
    "# 2、构建特征序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44858752",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['user_id','gender','age','occupation','zip','movie_id','genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e83399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>5780</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>2999</td>\n",
       "      <td>2639</td>\n",
       "      <td>1</td>\n",
       "      <td>958153068</td>\n",
       "      <td>White Boys (1999)</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>5851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1906</td>\n",
       "      <td>3368</td>\n",
       "      <td>5</td>\n",
       "      <td>957756608</td>\n",
       "      <td>One Little Indian (1973)</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>5938</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1266</td>\n",
       "      <td>2703</td>\n",
       "      <td>4</td>\n",
       "      <td>957273353</td>\n",
       "      <td>Five Wives, Three Secretaries and Me (1998)</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  gender  age  occupation   zip  movie_id  rating  timestamp  \\\n",
       "1000206     5780       2    2          18  2999      2639       1  958153068   \n",
       "1000207     5851       1    2          21  1906      3368       5  957756608   \n",
       "1000208     5938       2    3           2  1266      2703       4  957273353   \n",
       "\n",
       "                                               title  genres  \n",
       "1000206                            White Boys (1999)     240  \n",
       "1000207                     One Little Indian (1973)     192  \n",
       "1000208  Five Wives, Three Secretaries and Me (1998)     236  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# labelEncoder的说明 \n",
    "# https://zhuanlan.zhihu.com/p/33569866\n",
    "# 注意这个没有把对应的genres的多分类\n",
    "feature_max_idx = {}\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip() 函数是 Python 内置函数之一，它可以将多个序列（列表、元组、字典、集合、字符串以及 range() 区间\n",
    "#构成的列表）“压缩”成一个 zip 对象。所谓“压缩”，其实就是将这些序列中对应位置的元素重新组合，生成一个个新的元组。\n",
    "#http://c.biancheng.net/view/2237.html\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import random\n",
    "def gen_data_set(data, seq_max_len=50, negsample=0):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    data.sort_values('timestamp', inplace=True)\n",
    "    item_ids = data['movie_id'].unique()\n",
    "    #print(item_ids[:3])\n",
    "    item_id_genres_map = dict(zip(data['movie_id'].values, data['genres'].values))\n",
    "    #print(item_id_genres_map[858])\n",
    "    \n",
    "    #tqdm是一个方便且易于扩展的Python进度条，可以在python执行长循环时在命令行界面实时地显示一个进度提示信息，包括执行进度、处理速度等信息，且可在一定程度上进行定制。\n",
    "    for reviewerID, hist in data.groupby('user_id'):\n",
    "        pos_list = hist['movie_id'].tolist()\n",
    "        #print(pos_list[:3])\n",
    "        genres_list = hist['genres'].tolist()\n",
    "        rating_list = hist['rating'].tolist()\n",
    "        \n",
    "        #全局正样本中负采样\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list)*negsample)\n",
    "        for i in range(1, len(pos_list)):\n",
    "            #取到前i个\n",
    "            hist = pos_list[:i]\n",
    "            genres_hist = genres_list[:i]\n",
    "            seq_len = min(i, seq_max_len)\n",
    "            #https://blog.csdn.net/weixin_35757704/article/details/124037380\n",
    "            #[::]列表[起始:终止:步长]，默认是my_list[0:-1:1]\n",
    "            #[::3]等同于my_list[0:-1:3],步长为正从做往右，步长为负，从右往左\n",
    "            #hist[::-1][:seq_len] 逆序取了seq_len个\n",
    "            #如果不是最后一个\n",
    "            if i != len(pos_list) - 1:\n",
    "               train_set.append((reviewerID, pos_list[i], 1, \n",
    "                                hist[::-1][:seq_len],\n",
    "                                seq_len, \n",
    "                                genres_hist[::-1][:seq_len],\n",
    "                                genres_list[i],\n",
    "                                rating_list[i])) \n",
    "               for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, neg_list[i * negsample + negi], 0, \n",
    "                                hist[::-1][:seq_len],\n",
    "                                seq_len, \n",
    "                                genres_hist[::-1][:seq_len],\n",
    "                                item_id_genres_map[neg_list[i * negsample + negi]]))\n",
    "            else:#每一个用户的最后一个正样本用于测试\n",
    "                test_set.append((reviewerID, pos_list[i], 1, hist[::-1][:seq_len], seq_len, genres_hist[::-1][:seq_len],\n",
    "                                 genres_list[i],\n",
    "                                 rating_list[i]))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    print(len(train_set[0]),len(test_set[0]))\n",
    "                  \n",
    "    return train_set, test_set\n",
    "    \n",
    "SEQ_LEN = 50\n",
    "#这里设置为0， 用batch内负采样\n",
    "negsample = 0\n",
    "train_set, test_set = gen_data_set(data, SEQ_LEN, negsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77123e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile = data[['user_id','gender','age','occupation','zip']].drop_duplicates('user_id')\n",
    "item_profile  = data[['movie_id','genres']].drop_duplicates('movie_id')\n",
    "#最后还有一个参数 inplace=True 表示直接修改 df, 而不是返回新对象\n",
    "user_profile.set_index('user_id', inplace=True)\n",
    "#user_profile.index\n",
    "user_item_list = data.groupby('user_id')['movie_id'].apply(list)\n",
    "user_item_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa077b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def gen_model_input(train_set, user_profile, seq_max_len):\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    #print(train_uid[:3])\n",
    "    #[5111 1264 4867]\n",
    "    #正样本的movie_id\n",
    "    train_iid = np.array([line[1] for line in train_set])\n",
    "    train_label = np.array([line[2] for line in train_set])\n",
    "    train_seq = [line[3] for line in train_set]\n",
    "    #print(train_seq[:3])\n",
    "    #[[1900, 2826, 2872, 1975, 358, 3190, 1740, 1346, 1216, 2163, 3032, 3369, 990, 1928, 3043, 1223, 1970, 1179, 3220, 2711, 1088, 1007, 2719, 2054, 339, 67, 1049, 1122, 1343, 855, 2833, 1174, 964, 1201, 695, 1769, 3430, 1696, 1505, 1168, 1008, 867, 1085, 1194, 1341, 3315, 1178, 3272, 1706, 3135], [97, 1513, 2796, 1101, 1217, 157, 1047, 240, 1264, 1705, 2930, 1365, 1932, 2969, 418, 561, 1211, 3047, 2282, 495, 2923, 61, 3027, 1268, 1011, 1536, 2241, 1619, 1779, 243, 1934, 1638, 1303, 314, 2530, 1295, 1180, 470, 1783, 2079, 3248, 2722, 1357, 2890, 549, 1891, 3032, 2857, 2160, 2945], [1161, 859, 2880, 1251, 1067, 568, 50, 299, 1567, 288, 1100, 1203, 2160, 2476, 1202, 2806, 3435, 3198, 1296, 2097, 1009, 3511, 229, 1815, 2558, 2059, 3272, 2401, 2587, 32, 2006, 145, 1119, 1831, 1564, 1163, 1134, 860, 1198, 1990, 1213, 1060, 1111, 1105, 2786, 1085, 1114, 853, 2109, 852]]\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "    train_seq_genres = np.array([line[5] for line in train_set],dtype=list)\n",
    "    train_genres = np.array([line[6] for line in train_set])\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_seq_genres_pad = pad_sequences(train_seq_genres, maxlen=seq_max_len, padding='post', truncating='post',\n",
    "                                         value=0)\n",
    "    \n",
    "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
    "                    \"hist_genres\": train_seq_genres_pad,\n",
    "                    \"hist_len\": train_hist_len, \"genres\": train_genres}\n",
    "    for key in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "    return train_model_input, train_label\n",
    "    \n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_set[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70948044",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_input['user_id'][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a548c9",
   "metadata": {},
   "source": [
    "# 3、count #unique features for each sparse field and generate feature config for sequence feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/liboshi123/article/details/110550681\n",
    "#namedtuple()是产生具有命名字段的元组的工厂函数，namedtuple 比普通tuple具有更好的可读性，可以使代码更易于维护。同时与字典相比，又更加的轻量和高效。\n",
    "#namedtuple(typename, field_names,*,verbos=False, rename=Flase)\n",
    "#返回一个新类，名为typename\n",
    "\n",
    "#https://blog.csdn.net/Florine113/article/details/120988102\n",
    "#cls在python中表示类本身，self为类的一个实例。\n",
    "#cls可以返回类的一个实例。\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "\n",
    "DEFAULT_GROUP_NAME = \"default_group\"\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat',\n",
    "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'vocabulary_path', 'dtype', 'embeddings_initializer',\n",
    "                             'embedding_name',\n",
    "                             'group_name', 'trainable'])):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype=\"int32\", embeddings_initializer=None,\n",
    "                embedding_name=None,\n",
    "                group_name=DEFAULT_GROUP_NAME, trainable=True):\n",
    "\n",
    "        if embedding_dim == \"auto\":\n",
    "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
    "        if embeddings_initializer is None:\n",
    "            embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.0001, seed=2020)\n",
    "\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "\n",
    "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, vocabulary_path, dtype,\n",
    "                                              embeddings_initializer,\n",
    "                                              embedding_name, group_name, trainable)\n",
    "SparseFeat('user_id', feature_max_idx['user_id'], 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
    "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name', 'weight_name', 'weight_norm'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None, weight_name=None, weight_norm=True):\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name, weight_name,\n",
    "                                                    weight_norm)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparsefeat.vocabulary_size\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparsefeat.embedding_dim\n",
    "\n",
    "    @property\n",
    "    def use_hash(self):\n",
    "        return self.sparsefeat.use_hash\n",
    "\n",
    "    @property\n",
    "    def vocabulary_path(self):\n",
    "        return self.sparsefeat.vocabulary_path\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype\n",
    "\n",
    "    @property\n",
    "    def embeddings_initializer(self):\n",
    "        return self.sparsefeat.embeddings_initializer\n",
    "\n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparsefeat.embedding_name\n",
    "\n",
    "    @property\n",
    "    def group_name(self):\n",
    "        return self.sparsefeat.group_name\n",
    "\n",
    "    @property\n",
    "    def trainable(self):\n",
    "        return self.sparsefeat.trainable\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "    \n",
    "VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], 32,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype', 'transform_fn'])):\n",
    "    \"\"\" Dense feature\n",
    "    Args:\n",
    "        name: feature name.\n",
    "        dimension: dimension of the feature, default = 1.\n",
    "        dtype: dtype of the feature, default=\"float32\".\n",
    "        transform_fn: If not `None` , a function that can be used to transform\n",
    "        values of the feature.  the function takes the input Tensor as its\n",
    "        argument, and returns the output Tensor.\n",
    "        (e.g. lambda x: (x - 3.0) / 4.2).\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\", transform_fn=None):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype, transform_fn)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc46ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_genres', feature_max_idx['genres'], embedding_dim,\n",
    "                                embedding_name=\"genres\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim),\n",
    "                        SparseFeat('genres', feature_max_idx['genres'], embedding_dim)\n",
    "                       ]\n",
    "user_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5da120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "#https://blog.csdn.net/qq_29678299/article/details/89975667\n",
    "#dict的子类，用于计数\n",
    "train_counter = Counter(train_model_input['movie_id'])\n",
    "#train_counter， 以上求得movie_id分别对应的样本数\n",
    "#{525: 570,427: 1135,1505: 1363,956: 640,901: 334,....2073: 574}\n",
    "# train_counter.get(525,0) -> 570\n",
    "#item_count 次数的list【0，2174,.......】\n",
    "item_count = [train_counter.get(i,0) for i in range(item_feature_columns[0].vocabulary_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(\n",
    "    namedtuple('NegativeSampler', ['sampler', 'num_sampled', 'item_name', 'item_count', 'distortion'])):\n",
    "    \"\"\" NegativeSampler\n",
    "    Args:\n",
    "        sampler: sampler name,['inbatch', 'uniform', 'frequency' 'adaptive',] .\n",
    "        num_sampled: negative samples number per one positive sample.\n",
    "        item_name: pkey of item features .\n",
    "        item_count: global frequency of item .\n",
    "        distortion: skew factor of the unigram probability distribution.\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, sampler, num_sampled, item_name, item_count=None, distortion=1.0, ):\n",
    "        if sampler not in ['inbatch', 'uniform', 'frequency', 'adaptive']:\n",
    "            raise ValueError(' `%s` sampler is not supported ' % sampler)\n",
    "        if sampler in ['inbatch', 'frequency'] and item_count is None:\n",
    "            raise ValueError(' `item_count` must not be `None` when using `inbatch` or `frequency` sampler')\n",
    "        return super(NegativeSampler, cls).__new__(cls, sampler, num_sampled, item_name, item_count, distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52637afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意这里的item_count 是上面的list。\n",
    "sampler_config = NegativeSampler('inbatch',num_sampled=255,item_name=\"movie_id\",item_count=item_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52a619",
   "metadata": {},
   "source": [
    "# 4、搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218f1a4",
   "metadata": {},
   "source": [
    "Embedding 使用方法, tf.keras.layers.Embedding\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=False,\n",
    "    input_length=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd02fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python中OrderedDict用法， 有序的字典\n",
    "import collections\n",
    "def build_input_features(feature_columns, prefix=''):\n",
    "    input_features = collections.OrderedDict() #思考，这里为什么要用有序字典\n",
    "  \n",
    "    for fc in feature_columns:\n",
    "        if isinstance(fc, SparseFeat):\n",
    "            input_features[fc.name] = tf.keras.Input(\n",
    "                shape=(1,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "        elif isinstance(fc, DenseFeat):\n",
    "            input_features[fc.name] = tf.keras.Input(\n",
    "                shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "        elif isinstance(fc, VarLenSparseFeat):\n",
    "            input_features[fc.name] = tf.keras.Input(shape=(fc.maxlen,), name=prefix + fc.name,\n",
    "                                            dtype=fc.dtype)\n",
    "            if fc.weight_name is not None:\n",
    "                input_features[fc.weight_name] = tf.keras.Input(shape=(fc.maxlen, 1), name=prefix + fc.weight_name,\n",
    "                                                       dtype=\"float32\")\n",
    "            if fc.length_name is not None:\n",
    "                input_features[fc.length_name] = tf.keras.Input((1,), name=prefix + fc.length_name, dtype='int32')\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Invalid feature column type,got\", type(fc))\n",
    "\n",
    "    return input_features\n",
    "#测试\n",
    "build_input_features(user_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#表达式中使用if else,意思是if成立执行左边，否则执行右边\n",
    "def create_embedding_matrix(feature_columns, l2_reg, seed, prefix=\"sparse\", seq_mask_zero=True):\n",
    "    sparse_emb_dict = {}\n",
    "    \n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    \n",
    "    for feat in sparse_feature_columns:\n",
    "        emb = tf.keras.layers.Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
    "                        embeddings_initializer=feat.embeddings_initializer,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
    "                        name=prefix + '_emb_' + feat.embedding_name)\n",
    "        emb.trainable = feat.trainable\n",
    "        sparse_emb_dict[feat.embedding_name] = emb\n",
    "    \n",
    "    #varlen_sparse_feature 对比 sparse_feature， \n",
    "    #1、多了，mask_zero=seq_mask_zero\n",
    "    #2、名字不同\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "    \n",
    "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
    "        for feat in varlen_sparse_feature_columns:\n",
    "            # if feat.name not in sparse_embedding:\n",
    "            emb = tf.keras.layers.Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
    "                            embeddings_initializer=feat.embeddings_initializer,\n",
    "                            embeddings_regularizer=tf.keras.regularizers.L2(\n",
    "                                l2_reg),\n",
    "                            name=prefix + '_seq_emb_' + feat.name,\n",
    "                            mask_zero=seq_mask_zero)\n",
    "            emb.trainable = feat.trainable\n",
    "            sparse_emb_dict[feat.embedding_name] = emb\n",
    "            \n",
    "    return sparse_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162525c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "tensor = tf.ones_like(tensor) * (-2 ** 32 + 1)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WeightedSequenceLayer(weight_normalization=fc.weight_norm)(\n",
    "#                   [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])\n",
    "#其中\n",
    "#embedding_dict,varlen_embedding_lookup 中查找的经过变换embedding变换的output{'user_id': XXX, .......}\n",
    "#features, #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...] INPUT的张量\n",
    "#fc的属性 length_name='hist_len', weight_name=None, weight_norm=True\n",
    "\n",
    "#功能\n",
    "#如果fc.weight_name is  None:则直接用原来embedding的输出。\n",
    "#如果fc.weight_name is  not None，WeightedSequenceLayer加权，如果有feature_length_name, 则传入，否则不传入。三个，见例子\n",
    "#如果有feature_length_name，SequencePoolingLayer的supports_masking=False\n",
    "#如果没有feature_length_name，SequencePoolingLayer的supports_masking=True\n",
    "class WeightedSequenceLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"The WeightedSequenceLayer is used to apply weight score on variable-length sequence feature/multi-value feature.\n",
    "\n",
    "      Input shape\n",
    "        - A list of two  tensor [seq_value,seq_len,seq_weight]\n",
    "\n",
    "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)`` T应该是多少个值\n",
    "\n",
    "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
    "\n",
    "        - seq_weight is a 3D tensor with shape: ``(batch_size, T, 1)``\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, T, embedding_size)``.\n",
    "\n",
    "      Arguments\n",
    "        - **weight_normalization**: bool.Whether normalize the weight score before applying to sequence.\n",
    "\n",
    "        - **supports_masking**:If True,the input need to support masking.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_normalization=True, supports_masking=False, **kwargs):\n",
    "        super(WeightedSequenceLayer, self).__init__(**kwargs)\n",
    "        self.weight_normalization = weight_normalization\n",
    "        self.supports_masking = supports_masking\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(WeightedSequenceLayer, self).build(input_shape)# Be sure to call this somewhere!\n",
    "        if not self.supports_masking:\n",
    "            self.seq_len_max = int(input_shape[0][1])\n",
    "    def call(self, input_list, mask=None, **kwargs):\n",
    "        if self.supports_masking:\n",
    "            if mask is None:\n",
    "                raise ValueError(\"When supports_masking is true, please input mask\")\n",
    "            key_input, value_input = input_list\n",
    "            #expand_dims Returns a tensor with a length 1 axis inserted at index axis.\n",
    "            #tf.expand_dims(input, axis, name=None)\n",
    "            #image = tf.zeros([10,10,3])  \n",
    "            #tf.expand_dims(image, axis=0).shape.as_list()   [1,10,10,3] \n",
    "            #tf.expand_dims(image, axis=-1).shape.as_list()   [10,10,3,1]\n",
    "            mask = tf.expand_dims(mask[0], axis=2)\n",
    "        else :\n",
    "            #分别是embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]\n",
    "            #即feature对应的embedding的输出，'hist_len‘对应的input tensor， 以及 'weight_name'对应的input tensor\n",
    "            key_input, key_length_input, value_input = input_list\n",
    "            #sequence_mask, https://www.cnblogs.com/qianyuesheng/p/16445654.html\n",
    "            #输入是一个数，所以返回是一个shape(1,seq_len_max)的Tensor\n",
    "            mask = tf.sequence_mask(key_length_input,\n",
    "                                    self.seq_len_max, dtype=tf.bool)\n",
    "            #transpose， 把第二维和第三维的维度换一下。 比如原来shape是（batch_size, 1, seq_len_max）变成（batch_size, seq_len_max, 1）\n",
    "            mask = tf.transpose(mask, (0, 2, 1))\n",
    "            \n",
    "        embedding_size = key_input.shape[-1]\n",
    "            \n",
    "        if self.weight_normalization:\n",
    "            #Creates a tensor of all ones that has the same shape as the input.\n",
    "            #tf.ones_like(input, dtype=None, name=None)\n",
    "            #*表示乘号,**表示次方\n",
    "            #遗留问题，这里为什么又乘以（-2 ** 32 + 1）\n",
    "            paddings = tf.ones_like(value_input) * (-2 ** 32 + 1)\n",
    "        else :\n",
    "            paddings = tf.zeros_like(value_input)\n",
    "        \n",
    "        #操作'weight_name'对应的input tensor\n",
    "        value_input = tf.where(mask, value_input, paddings)\n",
    "        \n",
    "        if self.weight_normalization:\n",
    "            #tf.keras.layers.Softmax(axis=-1, **kwargs)(value_input) ,或者\n",
    "            #tf.keras.layers.Softmax(axis=-1, **kwargs)(value_input，mask) \n",
    "            value_input = tf.keras.layers.Softmax(value_input)\n",
    "        \n",
    "        if len(value_input.shape) == 2:\n",
    "            value_input = tf.expand_dims(value_input, axis=2)\n",
    "            #his operation creates a new tensor by replicating input multiples times. \n",
    "            #The output tensor's i'th dimension has input.dims(i) * multiples[i] elements, \n",
    "            #and the values of input are replicated multiples[i] times along the 'i'th dimension. For example, tiling [a b c d] by [2] produces [a b c d a b c d].\n",
    "            value_input = tf.tile(value_input, [1, 1, embedding_size])\n",
    "        \n",
    "        #A tensor, the element-wise product of the inputs. \n",
    "        return tf.multiply(key_input, value_input)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4777c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.array(np.random.randint(30,size=(5,3,2)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb58868",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(a, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61796548",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(a, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(a, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6dfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(a, axis=1, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(a, axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(a, axis=2, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ccd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(a, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd859c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(a, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(a, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b955bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(a, axis=1, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.math.reduce_sum(a, axis=1, keepdims=False)\n",
    "b = tf.expand_dims(b, axis=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e744a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(a, axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(a, axis=2, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#如果有weight，就对embedding进行了加权。 如果没有就是原来embedding的输出\n",
    "#如果原来有seq的len , SequencePoolingLayer(combiner, supports_masking=False)([seq_input, features[feature_length_name]])\n",
    "#否则supports_masking设置为true。 #SequencePoolingLayer(combiner, supports_masking=True)(seq_input)\n",
    "class SequencePoolingLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n",
    "\n",
    "      Input shape\n",
    "        - A list of two  tensor [seq_value,seq_len]\n",
    "\n",
    "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n",
    "\n",
    "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
    "\n",
    "      Arguments\n",
    "        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n",
    "\n",
    "        - **supports_masking**:If True,the input need to support masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n",
    "\n",
    "        if mode not in ['sum', 'mean', 'max']:\n",
    "            raise ValueError(\"mode must be sum or mean\")\n",
    "        self.mode = mode\n",
    "        self.eps = tf.constant(1e-8, tf.float32)\n",
    "        super(SequencePoolingLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = supports_masking\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.supports_masking:\n",
    "            self.seq_len_max = int(input_shape[0][1])\n",
    "        super(SequencePoolingLayer, self).build(\n",
    "            input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, seq_value_len_list, mask=None, **kwargs):\n",
    "        if self.supports_masking:\n",
    "            if mask is None:\n",
    "                raise ValueError(\n",
    "                    \"When supports_masking=True,input must support masking\")\n",
    "            uiseq_embed_list = seq_value_len_list\n",
    "            mask = tf.cast(mask, tf.float32)  # tf.to_float(mask)\n",
    "            user_behavior_length = reduce_sum(mask, axis=-1, keep_dims=True)\n",
    "            mask = tf.expand_dims(mask, axis=2)\n",
    "        else:#因为可能没有weighted，所以还是需要mask处理\n",
    "            #分别是上一层的输出，与长度\n",
    "            uiseq_embed_list, user_behavior_length = seq_value_len_list\n",
    "            #sequence_mask, https://www.cnblogs.com/qianyuesheng/p/16445654.html\n",
    "            #输入是一个数，所以返回是一个shape(1,seq_len_max)的Tensor\n",
    "            mask = tf.sequence_mask(user_behavior_length,\n",
    "                                    self.seq_len_max, dtype=tf.float32)\n",
    "            \n",
    "            #transpose， 把第二维和第三维的维度换一下。 比如原来shape是（batch_size, 1, seq_len_max）变成（batch_size, seq_len_max, 1）\n",
    "            mask = tf.transpose(mask, (0, 2, 1))\n",
    "\n",
    "        embedding_size = uiseq_embed_list.shape[-1]\n",
    "\n",
    "        mask = tf.tile(mask, [1, 1, embedding_size])\n",
    "\n",
    "        if self.mode == \"max\":\n",
    "            #mask原来有值为true， 否则为false\n",
    "            hist = uiseq_embed_list - (1 - mask) * 1e9\n",
    "            #This is the reduction operation for the elementwise tf.math.maximum op.\n",
    "            #>>> x = tf.constant([5, 1, 2, 4])>>> tf.reduce_max(x) <tf.Tensor: shape=(), dtype=int32, numpy=5>\n",
    "            #shape(batch_size, T, embedding_size)\n",
    "            return tf.math.reduce_max(hist, 1, keep_dims=True)\n",
    "        \n",
    "       \n",
    "        hist = tf.math.reduce_sum(uiseq_embed_list * mask, 1, keep_dims=True)\n",
    "\n",
    "        if self.mode == \"mean\":\n",
    "            hist = tf.math.reduce_mean(uiseq_embed_list * mask, keep_dims=True)\n",
    "\n",
    "        \n",
    "        return hist\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.supports_masking:\n",
    "            return (None, 1, input_shape[-1])\n",
    "        else:\n",
    "            return (None, 1, input_shape[0][-1])\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'mode': self.mode, 'supports_masking': self.supports_masking}\n",
    "        base_config = super(SequencePoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入\n",
    "#sparse_embedding_dict{'user_id': <keras.layers.core.embedding.Embedding>, .......}\n",
    "#sparse_input_dict, #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...] INPUT的张量\n",
    "#sparse_feature_columns： feature的各种配置\n",
    "def embedding_lookup(sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),\n",
    "                     mask_feat_list=(), to_list=False):\n",
    "    #defaultdict就可以避免这个错误，defaultdict的作用是在于，当字典里的element不存在但被查找时，返回的不是keyError而是一个默认值，\n",
    "    #这个默认值是什么呢,defaultdict(factory_function)这个factory_function可以是list、set、str等等，作用是当key不存在时，返回的是工厂函数的默认值，比如list对应[ ]\n",
    "    group_embedding_dict = collections.defaultdict(list)\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n",
    "            if fc.use_hash:#本例中都是false\n",
    "                lookup_idx = Hash(fc.vocabulary_size, mask_zero=(feature_name in mask_feat_list), vocabulary_path=fc.vocabulary_path)(\n",
    "                    sparse_input_dict[feature_name])\n",
    "            else:\n",
    "                lookup_idx = sparse_input_dict[feature_name] #返回了特征对应的input\n",
    "            #fc.group_name='default_group'\n",
    "            #各feature的input通过一层embedding层之后的输出，concat到一起\n",
    "            group_embedding_dict[fc.group_name].append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
    "    if to_list:\n",
    "        return list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699eb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入\n",
    "#sparse_embedding_dict{'user_id': <keras.layers.core.embedding.Embedding>, .......}\n",
    "#sparse_input_dict, #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...] INPUT的张量\n",
    "#sparse_feature_columns： feature的各种配置\n",
    "#和embedding_lookup最大的区别在于， 一个feature作为了一个group \n",
    "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if fc.use_hash:\n",
    "            lookup_idx = Hash(fc.vocabulary_size, mask_zero=True, vocabulary_path=fc.vocabulary_path)(sequence_input_dict[feature_name])\n",
    "        else:\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
    "    return varlen_embedding_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入\n",
    "#embedding_dict,varlen_embedding_lookup 中查找的经过变换embedding变换的output{'user_id': XXX, .......}\n",
    "#features, #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...] INPUT的张量\n",
    "#varlen_sparse_feature_columns： feature的各种配置\n",
    "#函数作用\n",
    "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns, to_list=False):\n",
    "    pooling_vec_list = defaultdict(list)\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        # 本例子中combiner='mean'， pooling的方式\n",
    "        combiner = fc.combiner\n",
    "        feature_length_name = fc.length_name\n",
    "        #如果fc.weight_name is  None:则直接用原来embedding的输出。\n",
    "        #如果fc.weight_name is  not None，WeightedSequenceLayer加权，如果有feature_length_name, 则传入，否则不传入。\n",
    "        # 如果有feature_length_name，SequencePoolingLayer的supports_masking=False\n",
    "        # 如果没有feature_length_name，SequencePoolingLayer的supports_masking=True\n",
    "        if feature_length_name is not None:\n",
    "            if fc.weight_name is not None:\n",
    "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm)(\n",
    "                    [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])\n",
    "            else:\n",
    "                seq_input = embedding_dict[feature_name]\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
    "                [seq_input, features[feature_length_name]])\n",
    "        else:\n",
    "            if fc.weight_name is not None:\n",
    "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm, supports_masking=True)(\n",
    "                    [embedding_dict[feature_name], features[fc.weight_name]])\n",
    "            else:\n",
    "                seq_input = embedding_dict[feature_name]\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
    "                seq_input)\n",
    "        pooling_vec_list[fc.group_name].append(vec)\n",
    "    if to_list:\n",
    "        return chain.from_iterable(pooling_vec_list.values())\n",
    "    return pooling_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da856cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features, #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...] INPUT的张量\n",
    "#feature_columns： feature的各种配置\n",
    "#dense相比于sparse没有经过embedding层，如果配置了fc.transform_fn，执行fc.transform_fn的结果作为输出\n",
    "def get_dense_input(features, feature_columns):\n",
    "    dense_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        if fc.transform_fn is None:\n",
    "            dense_input_list.append(features[fc.name])\n",
    "        else:\n",
    "            transform_result = Lambda(fc.transform_fn)(features[fc.name])\n",
    "            dense_input_list.append(transform_result)\n",
    "    return dense_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features, #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...] INPUT的张量\n",
    "#feature_columns： feature的各种配置\n",
    "#l2_reg_embedding=1e-6\n",
    "#embedding_matrix_dict{'user_id': <keras.layers.core.embedding.Embedding>, .......}\n",
    "def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,\n",
    "                               support_dense=True, support_group=False, embedding_matrix_dict=None):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "    if embedding_matrix_dict is None:\n",
    "        embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix,\n",
    "                                                        seq_mask_zero=seq_mask_zero)\n",
    "\n",
    "    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features, feature_columns)\n",
    "    if not support_dense and len(dense_value_list) > 0:\n",
    "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n",
    "    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n",
    "                                                                 varlen_sparse_feature_columns)\n",
    "    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n",
    "    if not support_group:\n",
    "        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict, dense_value_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892814f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DSSM(user_feature_columns, \n",
    "         item_feature_colums,\n",
    "         user_dnn_hidden_units=(64,42),\n",
    "         item_dnn_hidden_units=(64,32),\n",
    "         dnn_activation='relu',\n",
    "         dnn_use_bn=False,\n",
    "         l2_reg_dnn=0,\n",
    "         l2_reg_embedding=1e-6,\n",
    "         dnn_dropout=0,\n",
    "         loss_type='softmax',\n",
    "         temperature=0.05,\n",
    "         sampler_config=None,\n",
    "         seed=1024\n",
    "        ):\n",
    "    \"\"\"Instantiates the Deep Structured Semantic Model architecture.\n",
    "\n",
    "    :param user_feature_columns: An iterable containing user's features used by  the model.\n",
    "    :param item_feature_columns: An iterable containing item's features used by  the model.\n",
    "    :param user_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of user tower\n",
    "    :param item_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of item tower\n",
    "    :param dnn_activation: Activation function to use in deep net\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param loss_type: string. Loss type.\n",
    "    :param temperature: float. Scaling factor.\n",
    "    :param sampler_config: negative sample config.\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :return: A Keras model instance.\n",
    "\n",
    "    \"\"\"\n",
    "    #定义input\n",
    "    user_features = build_input_features(user_feature_columns)\n",
    "    #OrderedDict([('user_id',KerasTensor),('gender',KerasTensor)...]\n",
    "    user_inputs_list = list(user_features.values())\n",
    "    \n",
    "    #定义embedding\n",
    "    embedding_matrix_dict = create_embedding_matrix(user_feature_columns + item_feature_columns, l2_reg_embedding,\n",
    "                                                    seed=seed,\n",
    "                                                    seq_mask_zero=True)\n",
    "    print(embedding_matrix_dict)\n",
    "    #{'user_id': <keras.layers.core.embedding.Embedding>, .......}\n",
    "\n",
    "    \n",
    "    #sparse 非多值的embedding后的output进行contact, \n",
    "    #sparse 多值的特征embedding后经过WeightedSequenceLayer（可选），再进行get_varlen_pooling_list\n",
    "    #dense 不经过embedding , 可选的经过fc.transform_fn得到output\n",
    "    user_sparse_embedding_list, user_dense_value_list = input_from_feature_columns(user_features,\n",
    "                                                                                   user_feature_columns,\n",
    "                                                                                   l2_reg_embedding, seed=seed,\n",
    "                                                                                   embedding_matrix_dict=embedding_matrix_dict)\n",
    "    print(user_sparse_embedding_list)\n",
    "    print(user_dense_value_list)\n",
    "    \n",
    "    \n",
    "model = DSSM(user_feature_columns, item_feature_columns,user_dnn_hidden_units=(128,64, embedding_dim),\n",
    "             item_dnn_hidden_units=(64, embedding_dim,),loss_type='softmax',sampler_config=sampler_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575e884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5b083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
